{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All headlines from multiple sources have been stored in './headlines/headlines.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sources import news_sources  # Import the news_sources dictionary\n",
    "import csv\n",
    "\n",
    "# Function to fetch headlines based on the mapping\n",
    "def fetch_headlines(source_info):\n",
    "    try:\n",
    "        response = requests.get(source_info[\"url\"], allow_redirects=False)\n",
    "        if response.status_code in [301, 302]:  # Check if it's a redirect\n",
    "            print(f\"Redirect encountered for {source_info['url']}\")\n",
    "            return []\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extracting headlines based on the tags specified\n",
    "        headlines = []\n",
    "        for tag in source_info[\"headline_tags\"]:\n",
    "            headlines.extend(soup.find_all(tag))\n",
    "        return headlines\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to save the headlines and their URLs to a CSV file\n",
    "def save_headlines_to_csv(source_name, headlines, filename, base_url=None):\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as file:  # Open file with UTF-8 encoding\n",
    "        writer = csv.writer(file)\n",
    "        for headline in headlines:\n",
    "            headline_text = headline.get_text().strip()\n",
    "            href = headline.find('a')\n",
    "            if href and 'href' in href.attrs:\n",
    "                link = href['href']\n",
    "                if link.startswith('/'):  # Handle relative URLs\n",
    "                    link = base_url + link\n",
    "            else:\n",
    "                link = 'N/A'  # If no href found\n",
    "            writer.writerow([source_name, headline_text, link])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_name = './headlines/headlines.csv'\n",
    "\n",
    "    # Clear the file before writing new content\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Source\", \"Headline\", \"URL\"])  # Write the header row\n",
    "\n",
    "    for source_name, source_info in news_sources.items():\n",
    "        headlines = fetch_headlines(source_info)\n",
    "        save_headlines_to_csv(source_name, headlines, file_name, source_info[\"url\"])\n",
    "\n",
    "    print(f\"All headlines from multiple sources have been stored in '{file_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered headlines have been saved to './headlines/filtered_headlines.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def filter_headlines(input_filename, output_filename):\n",
    "    with open(input_filename, 'r', encoding='utf-8') as infile, open(output_filename, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        # Write the header to the output file\n",
    "        header = next(reader)\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Filter out single-word, double-word, and null headlines\n",
    "        for row in reader:\n",
    "            headline = row[1].strip()\n",
    "            word_count = len(headline.split())\n",
    "\n",
    "            if word_count > 2:  # Only keep headlines with more than 2 words\n",
    "                writer.writerow(row)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = './headlines/headlines.csv'\n",
    "    output_csv = './headlines/filtered_headlines.csv'\n",
    "\n",
    "    filter_headlines(input_csv, output_csv)\n",
    "\n",
    "    print(f\"Filtered headlines have been saved to '{output_csv}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled headlines have been saved to './headlines/labeled_headlines.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Keywords related to the stock market\n",
    "stock_keywords = [\n",
    "    'stock', 'market', 'wealth', 'buy', 'sell', 'invest', 'investment',\n",
    "    'trading', 'shares', 'equity', 'ipo', 'dividend', 'bonds', 'fund',\n",
    "    'portfolio', 'bull', 'bear', 'securities', 'capital', 'mutual',\n",
    "    'forex', 'commodities', 'derivatives', 'options', 'futures',\n",
    "    'indices', 'nasdaq', 'dow', 's&p', 'nyse', 'exchange', 'etf',\n",
    "    'hedge', 'hedge fund', 'margin', 'yield', 'interest rate', 'blue chip',\n",
    "    'growth stock', 'penny stock', 'short sell', 'long position', 'day trading',\n",
    "    'recession', 'economy', 'economic', 'earnings', 'valuation', 'assets',\n",
    "    'liabilities', 'balance sheet', 'cash flow', 'financial statement', 'quarterly results',\n",
    "    'insider trading', 'merger', 'acquisition', 'stock split', 'buyback',\n",
    "    'ipo', 'underwriting', 'public offering', 'private equity', 'venture capital',\n",
    "    'leverage', 'debt', 'credit rating', 'default', 'bankruptcy', 'inflation', 'deflation',\n",
    "    'federal reserve', 'central bank', 'interest rates', 'monetary policy', 'fiscal policy',\n",
    "    'gdp', 'gross domestic product', 'cpi', 'consumer price index', 'ppi', 'producer price index',\n",
    "    'yield curve', 'treasury', 'bond market', 'fixed income', 'junk bond', 'credit spread'\n",
    "]\n",
    "\n",
    "\n",
    "def label_headlines(input_filename, output_filename):\n",
    "    with open(input_filename, 'r', encoding='utf-8') as infile, open(output_filename, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        # Read and write the header with an additional \"Label\" column\n",
    "        header = next(reader)\n",
    "        header.append(\"Label\")\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Assign labels based on keywords\n",
    "        for row in reader:\n",
    "            headline = row[1].strip().lower()  # Assuming the headline is in the second column\n",
    "            label = 'general'  # Default label\n",
    "\n",
    "            # Check if any stock-related keyword is in the headline\n",
    "            if any(keyword in headline for keyword in stock_keywords):\n",
    "                label = 'stocks'\n",
    "\n",
    "            row.append(label)\n",
    "            writer.writerow(row)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = './headlines/filtered_headlines.csv'\n",
    "    output_csv = './headlines/labeled_headlines.csv'\n",
    "\n",
    "    label_headlines(input_csv, output_csv)\n",
    "\n",
    "    print(f\"Labeled headlines have been saved to '{output_csv}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with label 'general' have been removed. Cleaned data saved to 'cleaned_file.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('./headlines/labeled_headlines.csv')\n",
    "\n",
    "# Drop rows where the 'Label' column is 'general'\n",
    "df_cleaned = df[df['Label'] != 'general']\n",
    "df_cleaned = df_cleaned.dropna(subset=['URL'])\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_cleaned.to_csv('./headlines/cleaned_file.csv', index=False)\n",
    "\n",
    "print(f\"Rows with label 'general' have been removed. Cleaned data saved to 'cleaned_file.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'result' column has been added and the updated file has been saved as 'updated_file.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('./headlines/cleaned_file.csv')\n",
    "\n",
    "# Expanded keywords for buy and sell decisions\n",
    "buy_keywords = [\n",
    "    'invested', 'buy', 'acquire', 'acquisition', 'purchase', 'merger',\n",
    "    'expand', 'expansion', 'growth', 'invest', 'investment', 'partner',\n",
    "    'partnership', 'increase stake', 'joint venture', 'funding', 'raising capital',\n",
    "    'capital injection', 'backing', 'support', 'boost', 'increase holdings'\n",
    "]\n",
    "\n",
    "sell_keywords = [\n",
    "    'sell', 'divest', 'disinvest', 'offload', 'cut stake', 'exit',\n",
    "    'decrease stake', 'reduce holdings', 'liquidate', 'liquidation', 'spin-off',\n",
    "    'withdrawal', 'close down', 'shutdown', 'downsizing', 'layoffs', 'job cuts'\n",
    "]\n",
    "\n",
    "# Function to determine the result based on the headline\n",
    "def determine_result(headline):\n",
    "    headline_lower = headline.lower()  # Convert headline to lowercase for case-insensitive matching\n",
    "    if any(word in headline_lower for word in buy_keywords):\n",
    "        return 'Buy shares of the company'\n",
    "    elif any(word in headline_lower for word in sell_keywords):\n",
    "        return 'Sell shares of the company'\n",
    "    else:\n",
    "        return 'Hold/No action'\n",
    "\n",
    "# Apply the function to the 'Headline' column to create the 'result' column\n",
    "df['result'] = df['Headline'].apply(determine_result)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('./headlines/result_file.csv', index=False)\n",
    "\n",
    "print(\"The 'result' column has been added and the updated file has been saved as 'updated_file.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
